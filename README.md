# RAG App with NVIDIA NIM â€” Streamlit + LangChain

An endâ€‘toâ€‘end **RAG** application that performs fast, grounded Q\&A over **any pdf you upload**, powered by **NVIDIA NIM** for lowâ€‘latency inference and **LangChain** for orchestration.

> Repo: `Vatsal-Founder/RAG-app-with-Nvidia-NIM` (Python, GPLâ€‘3.0) â€” key files include `RAGnvidia.py` and `requirements.txt`.

---

## Features

* ðŸ“„ **Upload PDF research papers** and build a local vector index.
* ðŸ”Ž **Retrieve relevant passages** and generate **grounded answers** with citations.
* âš¡ **NVIDIA NIM** for fast chat/LLM inference via the NVIDIA API key.
* ðŸ§± **LangChain pipeline**: loader â†’ splitter â†’ embeddings â†’ vector store â†’ retriever â†’ LLM.
* ðŸ–¥ï¸ **Streamlit UI** for a simple, shareable demo.

Link: 
---

## Architecture

```
[Streamlit UI]
   â”œâ”€ Upload PDF(s)
   â”œâ”€ Ask questions
   â†“
[LangChain RAG]
   â”œâ”€ Load & split documents (chunks)
   â”œâ”€ Embed chunks (NIM or provider in code)
   â”œâ”€ Vector store (local FAISS or configured store)
   â”œâ”€ Retrieve topâ€‘k passages
   â””â”€ Generate answer (NVIDIA NIM chat model) â†’ citations
```

---

## Requirements

* Python **3.10+**
* **NVIDIA API key** (for NIM)

---

## Quick Start

### 1) Install

```bash
git clone https://github.com/Vatsal-Founder/RAG-app-with-Nvidia-NIM.git
cd RAG-app-with-Nvidia-NIM
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt
cp .env.example .env  # if present, or create your own
```

### 2) Configure environment

Create a `.env` file with at least:

```ini
# NVIDIA NIM
NVIDIA_API_KEY=your_nvidia_api_key
# Example model names (adjust to match your code)
NVIDIA_CHAT_MODEL=meta/llama-3.1-8b-instruct
NVIDIA_EMBEDDINGS_MODEL=nvidia/nv-embedqa-e5-v5

# Retrieval defaults (optional)
CHUNK_SIZE=1000
CHUNK_OVERLAP=150
TOP_K=5
```

> If your implementation sets model names directly in code, you can omit the model envs.

### 3) Run

```bash
streamlit run RAGnvidia.py
```

Open [http://localhost:8501](http://localhost:8501), upload a PDF, and start asking questions.

---

## Using the App

1. **Upload** one or more PDFs (research papers).
2. The app **splits** text, creates **embeddings**, and **indexes** them locally.
3. Ask a question. The retriever fetches the most relevant chunks; the NIMâ€‘backed LLM composes an answer with **citations/snippets**.

> Tip: For dense, technical papers, chunk size â‰ˆ **800â€“1200** with **120â€“200** overlap yields stable retrieval.

---

## Deployment (Optional)

### Docker

```dockerfile
FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
CMD ["streamlit", "run", "RAGnvidia.py", "--server.port", "8501", "--server.address", "0.0.0.0"]
```

Build & run:

```bash
docker build -t rag-nim .
docker run -p 8501:8501 --env-file .env rag-nim
```

### PaaS

* Command: `streamlit run RAGnvidia.py --server.port $PORT --server.address 0.0.0.0`
* Set `NVIDIA_API_KEY` in your platformâ€™s env/secrets.

---

## Configuration Tips

* **Models**: choose a fast instruct model for chat; pick an embeddings model optimized for retrieval.
* **Topâ€‘k**: start with 5; increase if answers miss context.
* **Citations**: surface source chunk/page numbers in the UI for transparency.

---

